@inproceedings{Anonymous2020AreTU,
title={Are Transformers universal approximators of sequence-to-sequence functions?},
author={Anonymous},
booktitle={Submitted to International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ByxRM0Ntvr},
note={under review}
}

@InProceedings{Anderson2017BottomUpAT,
author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
title = {Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@article{li2018generating,
  title={Generating diverse and accurate visual captions by comparative adversarial learning},
  author={Li, Dianqi and Huang, Qiuyuan and He, Xiaodong and Zhang, Lei and Sun, Ming-Ting},
  journal={arXiv preprint arXiv:1804.00861},
  year={2018}
}

@article{vinyals2016show,
  title={Show and tell: Lessons learned from the 2015 mscoco image captioning challenge},
  author={Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={4},
  pages={652--663},
  year={2016},
  publisher={IEEE}
}

@article{Ba2016LayerN,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@InProceedings{Biten2019GoodNews,
author = {Biten, Ali Furkan and Gomez, Lluis and Rusinol, Marcal and Karatzas, Dimosthenis},
title = {Good News, Everyone! Context Driven Entity-Aware Captioning for News Images},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@inproceedings{Bojar2018Findings,
    title = "Findings of the 2018 Conference on Machine Translation ({WMT}18)",
    author = "Bojar, Ond{\v{r}}ej  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Koehn, Philipp  and
      Monz, Christof",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6401",
    doi = "10.18653/v1/W18-6401",
    pages = "272--303",
}

@article{Cao2017VGGFace2AD,
    author       = {Cao, Qiong and Shen, Li and Xie, Weidi and Parkhi, Omkar M. and Zisserman, Andrew},
    year         = {2017},
    journal = {2018 13th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2018)},
    pages        = {67--74},
    title        = {{VGGFace2}: A Dataset for Recognising Faces across Pose and Age},
}

@article{Chen2015MicrosoftCC,
    author       = {Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{á}r, Piotr and Zitnick, C. Lawrence},
    year         = {2015},
    journal = {ArXiv},
    title        = {Microsoft {COCO} Captions: Data Collection and Evaluation Server},
    volume       = {abs/1504.00325},
}

@InProceedings{Cornia2019ShowCT,
author = {Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
title = {Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}


@InProceedings{Dauphin2017GLU,
  title = 	 {Language Modeling with Gated Convolutional Networks},
  author = 	 {Yann N. Dauphin and Angela Fan and Michael Auli and David Grangier},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {933--941},
  year = 	 {2017},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/dauphin17a/dauphin17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/dauphin17a.html},
  abstract = 	 {The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al. (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.}
}

@inproceedings{Denkowski2014Meteor,
    title = "Meteor Universal: Language Specific Translation Evaluation for Any Target Language",
    author = "Denkowski, Michael  and
      Lavie, Alon",
    booktitle = "Proceedings of the Ninth Workshop on Statistical Machine Translation",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-3348",
    doi = "10.3115/v1/W14-3348",
    pages = "376--380",
}

@inproceedings{Devlin2019BERT,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@InProceedings{Donahue2015LongTR,
author = {Donahue, Jeffrey and Anne Hendricks, Lisa and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
title = {Long-Term Recurrent Convolutional Networks for Visual Recognition and Description},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@inproceedings{Fan2018HierarchicalNS,
    title = "Hierarchical Neural Story Generation",
    author = "Fan, Angela  and
      Lewis, Mike  and
      Dauphin, Yann",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1082",
    doi = "10.18653/v1/P18-1082",
    pages = "889--898",
    abstract = "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
}

@InProceedings{Fang2015FromCT,
author = {Fang, Hao and Gupta, Saurabh and Iandola, Forrest and Srivastava, Rupesh K. and Deng, Li and Dollar, Piotr and Gao, Jianfeng and He, Xiaodong and Mitchell, Margaret and Platt, John C. and Lawrence Zitnick, C. and Zweig, Geoffrey},
title = {From Captions to Visual Concepts and Back},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@ARTICLE{Feng2013AutomaticCG,
author       = {Feng, Yansong and Lapata, Mirella},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Automatic Caption Generation for News Images},
year={2013},
volume={35},
number={4},
pages={797-812},
doi={10.1109/TPAMI.2012.118},
ISSN={},
month={April}
}

@article{Flesch1948,
  title={A new readability yardstick.},
  author={Flesch, Rudolph},
  journal={Journal of applied psychology},
  volume={32},
  number={3},
  pages={221},
  year={1948},
  publisher={American Psychological Association}
}

@inproceedings{Gao2019DeliberateAN,
  title={Deliberate Attention Networks for Image Captioning},
  author={Lianli Gao and Kaixuan Fan and Jingkuan Song and Xianglong Liu and Xing Xu and Heng Tao Shen},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2019},
  pages     = {8320--8327},
}

@inproceedings{Gardner2017AllenNLP,
    title = "{A}llen{NLP}: A Deep Semantic Natural Language Processing Platform",
    author = "Gardner, Matt  and
      Grus, Joel  and
      Neumann, Mark  and
      Tafjord, Oyvind  and
      Dasigi, Pradeep  and
      Liu, Nelson F.  and
      Peters, Matthew  and
      Schmitz, Michael  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2501",
    doi = "10.18653/v1/W18-2501",
    pages = "1--6",
    abstract = "Modern natural language processing (NLP) research requires writing code. Ideally this code would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the research. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.",
}

@InProceedings{Grave2016EfficientSA,
  title = 	 {Efficient softmax approximation for {GPU}s},
  author = 	 {{\'E}douard Grave and Armand Joulin and Moustapha Ciss{\'e} and David Grangier and Herv{\'e} J{\'e}gou},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1302--1310},
  year = 	 {2017},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/grave17a/grave17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/grave17a.html},
  abstract = 	 {We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computation time. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax. The code of our method is available at https://github.com/facebookresearch/adaptive-softmax.}
}

@InProceedings{He2016ResNet,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@unpublished{spacy2,
    AUTHOR = {Honnibal, Matthew and Montani, Ines},
    TITLE  = {{spaCy 2}: Natural language understanding with {B}loom embeddings, convolutional neural networks and incremental parsing},
    YEAR   = {2017},
    Note   = {To appear}
}

@InProceedings{Ioffe2015BatchNorm,
  title = 	 {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = 	 {Sergey Ioffe and Christian Szegedy},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {448--456},
  year = 	 {2015},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/ioffe15.html},
  abstract = 	 {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.}
}


@InProceedings{Karpathy2015DeepVA,
author = {Karpathy, Andrej and Fei-Fei, Li},
title = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@incollection{Kim2016MultimodalRL,
title = {Multimodal Residual Learning for Visual QA},
author = {Kim, Jin-Hwa and Lee, Sang-Woo and Kwak, Donghyun and Heo, Min-Oh and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
booktitle = {Advances in Neural Information Processing Systems 29},
pages = {361--369},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6446-multimodal-residual-learning-for-visual-qa.pdf}
}

@inproceedings{Kingma2015Adam,
title={Adam: A Method for Stochastic Optimization},
author={Diederik P. Kingma and Jimmy Ba},
booktitle={International Conference on Learning Representations},
year={2015},
}

@article{Kincaid1975DerivationON,
  title={Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel},
  author={Kincaid, J Peter and Fishburne Jr, Robert P and Rogers, Richard L and Chissom, Brad S},
  year={1975},
  publisher={Institute for Simulation and Training, University of Central Florida}
}


@article{Lample2019CrosslingualLM,
    author       = {Lample, Guillaume and Conneau, Alexis},
    year         = {2019},
    journal = {ArXiv},
    title        = {Cross-lingual Language Model Pretraining},
    volume       = {abs/1901.07291},
}

@article{Lan2019ALBERT,
    author       = {Lan, Zhen-Zhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
    year         = {2019},
    journal = {ArXiv},
    title        = {{ALBERT}: A Lite {BERT} for Self-supervised Learning of Language Representations},
    volume       = {abs/1909.11942},
}

@article{Li2019Boosted,
    author       = {Li, Jiangyun and Yao, Peng and Guo, Longteng and Zhang, Weicun},
    publisher    = {Multidisciplinary Digital Publishing Institute},
    year         = {2019},
    journal = {Applied Sciences},
    number       = {16},
    pages        = {3260},
    title        = {Boosted Transformer for Image Captioning},
    volume       = {9},
}

@inproceedings{Lin2004ROUGE,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W04-1013",
    pages = "74--81",
}

@inproceedings{Lin2014MicrosoftCC,
    author    = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge J. and Bourdev, Lubomir D. and Girshick, Ross B. and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{á}r, Piotr and Zitnick, C. Lawrence},
    booktitle = {ECCV},
    year      = {2014},
    title     = {Microsoft {COCO}: Common Objects in Context},
}

@article{Liu2019RoBERTaAR,
    author       = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar S. and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke S. and Stoyanov, Veselin},
    year         = {2019},
    journal = {ArXiv},
    title        = {{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
    volume       = {abs/1907.11692},
}

@inproceedings{Loshchilov2018DecoupledWD,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@InProceedings{Lu2017KnowingWT,
author = {Lu, Jiasen and Xiong, Caiming and Parikh, Devi and Socher, Richard},
title = {Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@inproceedings{Lu2018EntityAI,
    title = "Entity-aware Image Caption Generation",
    author = "Lu, Di  and
      Whitehead, Spencer  and
      Huang, Lifu  and
      Ji, Heng  and
      Chang, Shih-Fu",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1435",
    doi = "10.18653/v1/D18-1435",
    pages = "4013--4023",
    abstract = "Current image captioning approaches generate descriptions which lack specific information, such as named entities that are involved in the images. In this paper we propose a new task which aims to generate informative image captions, given images and hashtags as input. We propose a simple but effective approach to tackle this problem. We first train a convolutional neural networks - long short term memory networks (CNN-LSTM) model to generate a template caption based on the input image. Then we use a knowledge graph based collective inference algorithm to fill in the template with specific named entities retrieved via the hashtags. Experiments on a new benchmark dataset collected from Flickr show that our model generates news-style image descriptions with much richer information. Our model outperforms unimodal baselines significantly with various evaluation metrics.",
}

@incollection{Mikolov2013DistributedRO,
title = {Distributed Representations of Words and Phrases and their Compositionality},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
booktitle = {Advances in Neural Information Processing Systems 26},
pages = {3111--3119},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@inproceedings{Ott2019Fairseq,
    title = "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Baevski, Alexei  and
      Fan, Angela  and
      Gross, Sam  and
      Ng, Nathan  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-4009",
    doi = "10.18653/v1/N19-4009",
    pages = "48--53",
    abstract = "fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto",
}

@inproceedings{Paszke2017Automatic,
    author    = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
    booktitle = {NIPS Autodiff Workshop},
    year      = {2017},
    title     = {Automatic Differentiation in {PyTorch}},
}

@inproceedings{Papineni2002Bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@inproceedings{Pennington2014Glove,
    author    = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
    url       = {http://www.aclweb.org/anthology/D14-1162},
    booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
    year      = {2014},
    pages     = {1532--1543},
    title     = {{GloVe}: Global Vectors for Word Representation},
}

@inproceedings{Radford2019LanguageMA,
    author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    year   = {2019},
    title  = {Language Models are Unsupervised Multitask Learners},
}

@inproceedings{Rajpurkar2016SQuAD,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}

@inproceedings{Rajpurkar2018KnowWY,
    title = "Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}",
    author = "Rajpurkar, Pranav  and
      Jia, Robin  and
      Liang, Percy",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-2124",
    doi = "10.18653/v1/P18-2124",
    pages = "784--789",
    abstract = "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD achieves only 66{\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.",
}

@article{Ramisa2016BreakingNewsAA,
    author       = {Ramisa, Arnau and Yan, Fei and Moreno-Noguer, Francesc and Mikolajczyk, Krystian},
    year         = {2016},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    pages        = {1072--1085},
    title        = {BreakingNews: Article Annotation by Image and Text Processing},
    volume       = {40},
}

@article{Redmon2018YOLOv3AI,
  title={YOLOv3: An Incremental Improvement},
  author={Joseph Redmon and Ali Farhadi},
  journal={ArXiv},
  year={2018},
  volume={abs/1804.02767}
}

@inproceedings{Ren2015FasterRCNN,
    author    = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
    booktitle = {Advances in neural information processing systems},
    year      = {2015},
    pages     = {91--99},
    title     = {Faster r-cnn: Towards real-time object detection with region proposal networks},
}
@InProceedings{Rennie2017SelfCriticalST,
author = {Rennie, Steven J. and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
title = {Self-Critical Sequence Training for Image Captioning},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@inproceedings{See2017GetTT,
    title = "Get To The Point: Summarization with Pointer-Generator Networks",
    author = "See, Abigail  and
      Liu, Peter J.  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1099",
    doi = "10.18653/v1/P17-1099",
    pages = "1073--1083",
    abstract = "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
}

@inproceedings{Sennrich2015NeuralMT,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

@InProceedings{Schroff2015FaceNetAU,
author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
title = {FaceNet: A Unified Embedding for Face Recognition and Clustering},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@inproceedings{Sharma2018ConceptualCA,
    title = "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning",
    author = "Sharma, Piyush  and
      Ding, Nan  and
      Goodman, Sebastian  and
      Soricut, Radu",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1238",
    doi = "10.18653/v1/P18-1238",
    pages = "2556--2565",
    abstract = "We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.",
}

@article{Subramanian2019OnEA,
    author       = {Subramanian, Sandeep and Li, Raymond and Pilault, Jonathan and Pal, Christopher Joseph},
    year         = {2019},
    journal = {ArXiv},
    title        = {On Extractive and Abstractive Neural Document Summarization with Transformer Language Models},
    volume       = {abs/1909.03186},
}

@ARTICLE{Tariq2017ACE,
author={A. {Tariq} and H. {Foroosh}},
journal={IEEE Transactions on Image Processing},
title={A Context-Driven Extractive Framework for Generating Realistic Image Descriptions},
year={2017},
volume={26},
number={2},
pages={619-632},
keywords={image annotation;image retrieval;meta data;probability;context-driven extractive framework;realistic image description generation;automatic image annotation;image search;image retrieval;organization systems;semantic concepts;visual features;semantic gap;annotation systems;contextual cues;artificial ground truth descriptions;data collection;image descriptions;real-world data set;news image captions;auxiliary information sources;metadata;probability space;annotation prediction;Context;Visualization;Training;Semantics;Estimation;Adaptation models;Vocabulary;Textual image description;context discovery;image semantics;heterogeneous information fusion},
doi={10.1109/TIP.2016.2628585},
ISSN={},
month={Feb},}

@article{Templin1957CertainLS,
  title={Certain language skills in children; their development and interrelationships.},
  author={Templin, Mildred C},
  year={1957},
  publisher={University of Minnesota Press}
}

@inproceedings{Tenney2019BertRT,
    title = "{BERT} Rediscovers the Classical {NLP} Pipeline",
    author = "Tenney, Ian  and
      Das, Dipanjan  and
      Pavlick, Ellie",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1452",
    doi = "10.18653/v1/P19-1452",
    pages = "4593--4601",
    abstract = "Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",
}

@incollection{Vaswani2017AttentionIA,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@InProceedings{Vedantam2015CIDEr,
author = {Vedantam, Ramakrishna and Lawrence Zitnick, C. and Parikh, Devi},
title = {CIDEr: Consensus-Based Image Description Evaluation},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@InProceedings{Vinyals2015ShowAT,
author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
title = {Show and Tell: A Neural Image Caption Generator},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@inproceedings{Wang2019GLUE,
    author    = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
    url       = {https://openreview.net/forum?id=rJ4km2R5t7},
    booktitle = {International Conference on Learning Representations},
    year      = {2019},
    title     = {{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
}

@article{Wang2019SuperGLUEAS,
    author       = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
    year         = {2019},
    journal = {ArXiv},
    title        = {{SuperGLUE}: A Stickier Benchmark for General-Purpose Language Understanding Systems},
    volume       = {abs/1905.00537},
}

@inproceedings{Wang2019Hierarchical,
    author    = {Wang, Weixuan and Chen, Zhihong and Hu, Haifeng},
    booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
    year      = {2019},
    pages     = {8957--8964},
    title     = {Hierarchical attention network for image captioning},
    volume    = {33},
}

@inproceedings{Wu2018PayLA,
    author    = {Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann and Auli, Michael},
    url       = {https://openreview.net/forum?id=SkVhlh09tX},
    booktitle = {International Conference on Learning Representations},
    year      = {2019},
    title     = {Pay Less Attention with Lightweight and Dynamic Convolutions},
}


@InProceedings{Xie2017AggregatedRT,
author = {Xie, Saining and Girshick, Ross and Dollar, Piotr and Tu, Zhuowen and He, Kaiming},
title = {Aggregated Residual Transformations for Deep Neural Networks},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@inproceedings{Xu2015ShowAA,
    author    = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron C. and Salakhutdinov, Ruslan and Zemel, Richard S. and Bengio, Yoshua},
    booktitle = {ICML},
    year      = {2015},
    title     = {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
}

@article{Yang2019XLNetGA,
    author       = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime G. and Salakhutdinov, Ruslan and Le, Quoc V.},
    year         = {2019},
    journal = {ArXiv},
    title        = {{XLNet}: Generalized Autoregressive Pretraining for Language Understanding},
    volume       = {abs/1906.08237},
}

@InProceedings{You2016ImageCW,
author = {You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
title = {Image Captioning With Semantic Attention},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@article{Young2014FromID,
    title = "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
    author = "Young, Peter  and
      Lai, Alice  and
      Hodosh, Micah  and
      Hockenmaier, Julia",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "2",
    year = "2014",
    url = "https://www.aclweb.org/anthology/Q14-1006",
    doi = "10.1162/tacl_a_00166",
    pages = "67--78",
    abstract = "We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.",
}

@article{Zhang2016JointFD,
    author       = {Zhang, Kaipeng and Zhang, Zhanpeng and Li, Zhifeng and Qiao, Yu},
    year         = {2016},
    journal = {IEEE Signal Processing Letters},
    pages        = {1499--1503},
    title        = {Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks},
    volume       = {23},
}

@inproceedings{Zhao2019InformativeIC,
    title = "Informative Image Captioning with External Sources of Information",
    author = "Zhao, Sanqiang  and
      Sharma, Piyush  and
      Levinboim, Tomer  and
      Soricut, Radu",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1650",
    doi = "10.18653/v1/P19-1650",
    pages = "6485--6494",
    abstract = "An image caption should fluently present the essential information in a given image, including informative, fine-grained entity mentions and the manner in which these entities interact. However, current captioning models are usually trained to generate captions that only contain common object names, thus falling short on an important {``}informativeness{''} dimension. We present a mechanism for integrating image information together with fine-grained labels (assumed to be generated by some upstream models) into a caption that describes the image in a fluent and informative manner. We introduce a multimodal, multi-encoder model based on Transformer that ingests both image features and multiple sources of entity labels. We demonstrate that we can learn to control the appearance of these entity labels in the output, resulting in captions that are both fluent and informative.",
}

@article{Zhu2018CaptioningTW,
    author       = {Zhu, Xinxin and Li, Lixiang and Liu, Jing and Peng, Haipeng and Niu, Xinxin},
    publisher    = {Multidisciplinary Digital Publishing Institute},
    year         = {2018},
    journal = {Applied Sciences},
    number       = {5},
    pages        = {739},
    title        = {Captioning transformer with stacked attention modules},
    volume       = {8},
}

@article{Gan2017Semantic,
	abstract = {A Semantic Compositional Network (SCN) is developed for image
	captioning, in which semantic concepts (i.e., tags) are detected from the
	image, and the probability of each tag is used to compose the parameters in
	a long short-term mem-ory (LSTM) network. The SCN extends each weight
	matrix of the LSTM to an ensemble of tag-dependent weight matrices. The
	degree to which each member of the ensemble is used to generate an image
	caption is tied to the image-dependent probability of the corresponding
	tag. In addition to caption-ing images, we also extend the SCN to generate
	captions for video clips. We qualitatively analyze semantic composition in
	SCNs, and quantitatively evaluate the algorithm on three benchmark
	datasets: COCO, Flickr30k, and Youtube2Text. Experimental results show that
	the proposed method signifi-cantly outperforms prior state-of-the-art
	approaches, across multiple evaluation metrics.},
	author = {Gan, Zhe and Gan, Chuang and He, Xiaodong and Pu, Yunchen and
	Tran, Kenneth and Gao, Jianfeng and Carin, Lawrence and Deng, Li and
	University, Duke},
	journal = {Computer Vision and Pattern Recognition},
	title = {{{Semantic Compositional Networks for Visual Captioning}}},
	url = {https://arxiv.org/pdf/1611.08002.pdf},
	year = {2017}
}

@article{Wu2016HighLevel,
	abstract = {Much of the recent progress in Vision-to-Language (V2L)
	problems has been achieved through a combination of Convolutional Neural
	Networks (CNNs) and Recurrent Neural Networks (RNNs). This approach does
	not explicitly represent high-level semantic concepts, but rather seeks to
	progress directly from image features to text. We propose here a method of
	incorporating high-level concepts into the very successful CNN-RNN
	approach, and show that it achieves a significant improvement on the
	state-of-the-art performance in both image captioning and visual question
	answering. We also show that the same mechanism can be used to introduce
	external semantic information and that doing so further improves
	performance. In doing so we provide an analysis of the value of high level
	semantic information in V2L problems.},
	author = {Wu, Qi and Shen, Chunhua and Liu, Lingqiao and Dick, Anthony and
	van den Hengel, Anton},
	journal = {Computer Vision and Pattern Recognition},
	title = {{{What Value Do Explicit High Level Concepts Have in Vision to
	Language
	Problems?}}},
	url = {http://arxiv.org/abs/1506.01144},
	year = {2016}
}
